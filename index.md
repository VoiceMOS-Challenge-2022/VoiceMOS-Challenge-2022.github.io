## The VoiceMOS Challenge 2022

Human listening tests are the gold standard for evaluating synthesized speech.  Objective measures of speech quality have low correlation with human ratings, and the generalization abilities of current data-driven quality prediction systems suffer significantly from domain mismatch.  The VoiceMOS Challenge aims to encourage research in the area of automatic prediction of Mean Opinion Scores (MOS) for synthesized speech.   This challenge has two tracks:

 * Main track: We recently collected a large-scale dataset of MOS ratings for a large variety of text-to-speech and voice conversion systems spanning many years, and this challenge releases this data to the public for the first time as the main track dataset.
 * Out-of-domain track: The data for this track comes from a different listening test from the main track.  The purpose of this track is to study the generalization ability of proposed MOS prediction models to a different listening test context.  A smaller amount of labeled data is made available to participants, and unlabeled audio samples from the same listening test are made available as well, to encourage exploration of unsupervised and semi-supervised approaches.

Participation is open to all.  The main track is required for all participants, and the out-of-domain track is optional.  Participants in the challenge are strongly encouraged to submit papers to the special session.  The focus of the special session is on understanding and comparing MOS prediction techniques using a standardized dataset.

## Participate

To participate in the challenge, you need to complete **both** of these two steps:

1. Make an account on CodaLab and register for the challenge [here](https://codalab.lisn.upsaclay.fr/competitions/695).  Please only register one account for your team.  Please also be patient as registration needs to be manually approved; we will try to approve all registrations within a day.

2. Fill out [this Google Form here](https://docs.google.com/forms/d/e/1FAIpQLSfvXt5hLRmiICN2SwmInWAAStgCtwC6a8XyzITZ6bt-2gt1HQ/viewform) about your team.
  We use this information to match your CodaLab account to your team.
  
Once your account is approved on CodaLab, you will be able to see information about how to download the data, etc.

## Schedule

The schedule for the challenge is as follows:

* Release of main track and out-of-domain training data: current
* Release of evaluation data / start of test phase: February 21, 2022
* Test phase results submission deadline: February 28, 2022
* Results sent to participant: March 7, 2022
* Interspeech Paper submission deadline: March 21, 2022

## Organizers

* Wen-Chin Huang (Nagoya University, Japan)
* Erica Cooper (National Institute of Informatics, Japan)
* Yu Tsao (Academia Sinica, Taiwan)
* Hsin-Min Wang (Academia Sinica, Taiwan)
* Tomoki Toda (Nagoya University, Japan)
* Junichi Yamagishi (National Institute of Informatics, Japan)

